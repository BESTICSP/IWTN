
# Spatially-Correlative Loss

[arXiv](https://arxiv.org/abs/2104.00854) | [website]()
<br>

We provide our Pytorch implementation of "The Spatially-Correlative Loss for Various Image Translation Tasks".

[The Spatially-Correlative Loss for Various Image Translation Tasks](https://arxiv.org/abs/2104.00854) <br>
[Chuanxia Zheng](http://www.chuanxiaz.com), [Tat-Jen Cham](http://www.ntu.edu.sg/home/astjcham/), [Jianfei Cai](https://research.monash.edu/en/persons/jianfei-cai) <br>
NTU and Monash University <br>
In CVPR2021 <br>

## Example Results

### Unpaired Image-to-Image Translation
<img src='images/unpairedI2I-translation.gif' align="center">

### Single Image Translation
<img src='images/single-translation.gif' align="center">

## [More results on project page]()

## Getting Started

### Installation

### Datasets

### Training

### Testing

### Pretrained Models

## Citation
```
@inproceedings{zheng2019pluralistic,
  title={The Spatially-Correlative Loss for Various Image Translation Tasks},
  author={Zheng, Chuanxia and Cham, Tat-Jen and Cai, Jianfei},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2021}
}
```

## Acknowledge
Our code is developed based on [CUT](https://github.com/taesungp/contrastive-unpaired-translation) and [CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix). We also thank [pytorch-fid](https://github.com/mseitzer/pytorch-fid) for FID computation,  [LPIPS](https://github.com/richzhang/PerceptualSimilarity) for diversity score, and [D&C](https://github.com/clovaai/generative-evaluation-prdc) for density and coverage evaluation.




